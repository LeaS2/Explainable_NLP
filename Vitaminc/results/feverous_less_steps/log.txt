03/19/2023 11:22:22 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
03/19/2023 11:22:22 - INFO - __main__ -   Training/evaluation parameters VitCTrainingArgs(output_dir='results/feverous_less_steps', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=128, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=20000, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Mar19_11-22-22_serv-9205', logging_first_step=False, logging_steps=500, save_steps=4000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='results/feverous_less_steps', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, eval_all_checkpoints=True, do_test=True, test_on_best_ckpt=True)
03/19/2023 11:22:23 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0pnsrp86
03/19/2023 11:22:23 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/41fd59466159d48786110897b3d780f83acfbc5bc8282ab17cf888a3aecbef49.8b3d8d4ae5da2b27fc470a9914f3e870cf8a87f573b71c603ce66be72f09abe7
03/19/2023 11:22:23 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/41fd59466159d48786110897b3d780f83acfbc5bc8282ab17cf888a3aecbef49.8b3d8d4ae5da2b27fc470a9914f3e870cf8a87f573b71c603ce66be72f09abe7
03/19/2023 11:22:23 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/41fd59466159d48786110897b3d780f83acfbc5bc8282ab17cf888a3aecbef49.8b3d8d4ae5da2b27fc470a9914f3e870cf8a87f573b71c603ce66be72f09abe7
03/19/2023 11:22:23 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "vitc_fever/checkpoint-50000",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 11:22:24 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/41fd59466159d48786110897b3d780f83acfbc5bc8282ab17cf888a3aecbef49.8b3d8d4ae5da2b27fc470a9914f3e870cf8a87f573b71c603ce66be72f09abe7
03/19/2023 11:22:24 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "vitc_fever/checkpoint-50000",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "vitc_real",
    "vitc_synthetic",
    "fever"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "SUPPORTS",
    "1": "REFUTES",
    "2": "NOT ENOUGH INFO"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "NOT ENOUGH INFO": "2",
    "REFUTES": "1",
    "SUPPORTS": "0"
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 11:22:24 - INFO - transformers.tokenization_utils_base -   Model name 'tals/albert-base-vitaminc-fever' not found in model shortcut name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). Assuming 'tals/albert-base-vitaminc-fever' is a path, a model identifier, or url to a directory containing tokenizer files.
03/19/2023 11:22:24 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp08dkcf2y
03/19/2023 11:22:25 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/7aacaf1e4ae6e087e966a814e6071a2bf1c80be24f4c0cc57fa0907242ea68e4.1f8b90a03f83ee30ea32a414a46d417a91d5ed797e90664b02b7591531a8e0e9
03/19/2023 11:22:25 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/7aacaf1e4ae6e087e966a814e6071a2bf1c80be24f4c0cc57fa0907242ea68e4.1f8b90a03f83ee30ea32a414a46d417a91d5ed797e90664b02b7591531a8e0e9
03/19/2023 11:22:27 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpuncbkef6
03/19/2023 11:22:27 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/8c82e8d7c3bcf544d301b8d4777d1b6dd7b700c7f58c955f19d289a1f70eceeb.623993453f3f6b9f6ad831899812f482e5cde100e664124feb3a6446d69a26bf
03/19/2023 11:22:27 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/8c82e8d7c3bcf544d301b8d4777d1b6dd7b700c7f58c955f19d289a1f70eceeb.623993453f3f6b9f6ad831899812f482e5cde100e664124feb3a6446d69a26bf
03/19/2023 11:22:28 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsypsuc_s
03/19/2023 11:22:28 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/3a40dcef47d1d3a91cda05c752bbcd6206d3b3f9d51c0de439adc38b652da49d.7f7ace06b0faaa16d30d7aba993112e16d81588fd5afa15b88262da4af208014
03/19/2023 11:22:28 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/3a40dcef47d1d3a91cda05c752bbcd6206d3b3f9d51c0de439adc38b652da49d.7f7ace06b0faaa16d30d7aba993112e16d81588fd5afa15b88262da4af208014
03/19/2023 11:22:28 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/7aacaf1e4ae6e087e966a814e6071a2bf1c80be24f4c0cc57fa0907242ea68e4.1f8b90a03f83ee30ea32a414a46d417a91d5ed797e90664b02b7591531a8e0e9
03/19/2023 11:22:28 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/tokenizer.json from cache at None
03/19/2023 11:22:28 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/added_tokens.json from cache at None
03/19/2023 11:22:28 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/8c82e8d7c3bcf544d301b8d4777d1b6dd7b700c7f58c955f19d289a1f70eceeb.623993453f3f6b9f6ad831899812f482e5cde100e664124feb3a6446d69a26bf
03/19/2023 11:22:28 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/3a40dcef47d1d3a91cda05c752bbcd6206d3b3f9d51c0de439adc38b652da49d.7f7ace06b0faaa16d30d7aba993112e16d81588fd5afa15b88262da4af208014
03/19/2023 11:22:29 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpeva5rbbj
03/19/2023 11:22:32 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bc146aa70c6b970fc726f903e34f2b06b914886a365409e34b9fd738807ecfc7.36d94c7865a57ac31a6ac22b015e6400e842e3bec8b7ab9a68a001ee2ac8a367
03/19/2023 11:22:32 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/bc146aa70c6b970fc726f903e34f2b06b914886a365409e34b9fd738807ecfc7.36d94c7865a57ac31a6ac22b015e6400e842e3bec8b7ab9a68a001ee2ac8a367
03/19/2023 11:22:32 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bc146aa70c6b970fc726f903e34f2b06b914886a365409e34b9fd738807ecfc7.36d94c7865a57ac31a6ac22b015e6400e842e3bec8b7ab9a68a001ee2ac8a367
03/19/2023 11:22:33 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/19/2023 11:22:33 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at tals/albert-base-vitaminc-fever.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/19/2023 11:22:33 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Collecting train examples (ratio=1.0) from dataset file at data/feverous
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   guid: 14802
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 14, 641, 25, 7112, 19, 11547, 16174, 15, 2094, 1583, 37, 14, 636, 2558, 4734, 6931, 1, 5060, 1, 4734, 6931, 554, 500, 500, 16, 19695, 155, 15, 10675, 16, 636, 2558, 1218, 1, 43, 22233, 1, 1218, 19695, 500, 500, 9, 14, 13, 22976, 641, 16, 5919, 25, 21, 538, 675, 17, 538, 8583, 527, 4080, 3801, 131, 156, 641, 601, 16, 6515, 17, 12601, 3287, 15, 283, 16, 5919, 9, 14, 13, 22976, 641, 16, 5919, 23, 613, 28, 14, 13, 22976, 641, 16, 342, 2964, 19, 636, 2558, 43, 22233, 1, 43, 22233, 500, 500, 19, 3439, 34, 2116, 16, 3860, 2544, 15, 17, 2433, 19, 2249, 9, 636, 2558, 1307, 4688, 1, 6482, 210, 1, 2793, 49, 1, 1307, 4688, 63, 210, 1685, 49, 500, 500, 15, 21, 1525, 3860, 3853, 17, 21352, 16, 636, 2558, 27962, 1, 27962, 500, 500, 257, 40, 681, 597, 19, 3544, 48, 641, 9, 13, 3, 13, 22976, 641, 16, 5919, 5, 1682, 43, 19, 11547, 16174, 6, 25, 21, 538, 675, 1165, 2433, 19, 2249, 15, 10234, 63, 210, 1685, 49, 257, 40, 681, 597, 19, 82, 3544, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   guid: 70296
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 14, 981, 23, 15048, 34, 14, 1314, 13, 19410, 2916, 657, 20642, 15, 72, 1537, 2081, 26, 32, 20, 4996, 21, 78, 505, 16, 4885, 6635, 108, 856, 30, 23, 52, 13, 29501, 27, 14, 895, 19348, 17, 10908, 1284, 720, 2442, 1598, 29, 4885, 6635, 108, 856, 19, 1029, 9, 14, 981, 23, 976, 26519, 1333, 34, 14, 941, 15, 8299, 21, 3682, 28511, 28, 461, 304, 72, 41, 529, 66, 13, 7, 177, 7939, 18, 7, 17, 46, 1864, 26, 636, 2558, 7426, 1, 1041, 1, 18910, 1, 18910, 22, 18, 16534, 2208, 1684, 500, 500, 17, 13841, 103, 1880, 3038, 9, 14, 6153, 13, 7426, 16, 1095, 641, 28, 21, 829, 16, 636, 2558, 7563, 8, 99, 5119, 1130, 1, 2407, 38, 20901, 1, 7563, 8, 99, 5119, 1130, 19891, 500, 500, 636, 2558, 20148, 1, 2407, 38, 20901, 1, 5487, 1, 5487, 18, 500, 500, 17, 636, 2558, 5739, 1, 1041, 1, 16549, 2829, 1, 779, 1, 4102, 1, 6899, 11557, 10821, 1, 4542, 201, 28515, 500, 500, 25, 1727, 20, 1816, 9, 13, 3, 11893, 7852, 99, 304, 23, 15048, 34, 13, 19410, 2916, 657, 20642, 17, 23, 21, 3682, 28511, 28, 28, 461, 304, 72, 41, 529, 66, 13, 7, 177, 7939, 18, 7, 17, 46, 1864, 26, 1029, 22, 18, 16534, 2208, 1684, 56, 25, 1121, 397, 20, 183, 201, 28515, 17, 708, 19891, 1684, 497, 89, 564, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   guid: 16578
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 13, 22, 21408, 256, 22, 6, 25, 40, 3754, 136, 636, 2558, 14789, 1, 5, 7962, 3019, 6, 1, 14789, 500, 500, 19, 8748, 636, 2558, 2681, 1, 7962, 1664, 49, 1, 5788, 1, 2681, 16035, 136, 500, 500, 15, 636, 2558, 7962, 3019, 1, 7962, 3019, 500, 500, 9, 13, 3, 26600, 108, 15, 21, 3534, 9, 918, 3020, 401, 13, 5, 918, 9, 3399, 1306, 4444, 534, 6, 476, 335, 19, 78, 16035, 136, 15, 998, 15, 21, 475, 19, 342, 2394, 15, 63, 21, 600, 350, 16, 13, 12081, 15, 518, 3516, 19, 356, 1110, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   guid: 1196
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 7418, 2159, 994, 58, 22604, 23, 10099, 19, 253, 1593, 436, 34, 636, 2558, 29253, 18, 1, 29253, 18, 500, 12660, 6956, 1678, 17, 1968, 5559, 18, 57, 74, 216, 9, 13, 3, 115, 14, 64, 733, 18, 1568, 54, 326, 7310, 1532, 12792, 32, 15, 7418, 2159, 994, 58, 23, 213, 20, 19, 793, 22, 18, 19, 1593, 436, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   guid: 5407
03/19/2023 11:22:48 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 20, 2408, 1153, 15205, 20, 3528, 4713, 13, 5, 381, 285, 390, 15, 2309, 28, 20, 3528, 4713, 2832, 921, 6, 25, 21, 336, 636, 2558, 1666, 1293, 1, 1666, 1293, 500, 500, 11199, 37, 636, 2558, 13855, 9685, 1, 13855, 9685, 500, 500, 15, 636, 2558, 18910, 1, 18910, 500, 500, 9, 24, 117, 33, 893, 19, 1752, 15, 17, 75, 151, 122, 19, 14, 987, 6309, 24, 664, 14, 371, 460, 26, 14, 64, 85, 19, 973, 9, 24, 23, 1292, 20, 10097, 19, 327, 542, 75, 40, 3193, 34, 14, 636, 2558, 18910, 1, 1666, 1293, 1, 29107, 1, 18910, 3907, 111, 607, 500, 500, 216, 61, 4756, 16, 636, 2558, 12280, 8, 18594, 68, 1, 108, 1, 17056, 1, 1666, 1293, 1, 12280, 8, 18594, 68, 500, 500, 9, 13, 3, 3907, 111, 11199, 20, 2408, 1153, 15205, 20, 3528, 4713, 4164, 730, 8, 18594, 68, 15, 3119, 33, 545, 19, 542, 30, 373, 19, 1752, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/19/2023 11:22:59 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Saving features into cached file data/cached_train_AlbertTokenizerFast_256_feverous_all [took 10.768 s]
03/19/2023 11:22:59 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Collecting dev examples (ratio=1.0) from dataset file at data/feverous
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   guid: 3044
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 190, 1244, 25, 40, 189, 5784, 13435, 679, 34, 636, 2558, 18, 6268, 1, 6893, 3700, 7437, 1, 18, 6268, 1572, 3700, 7437, 500, 500, 26, 14, 636, 2558, 18219, 1, 24162, 68, 1, 28359, 1, 18219, 3647, 237, 500, 500, 30, 4850, 27, 320, 1538, 15, 1247, 9, 179, 82, 5421, 15, 190, 1244, 63, 74, 2525, 10410, 9, 13, 3, 190, 1244, 25, 40, 189, 5784, 13435, 30, 967, 355, 407, 430, 4977, 15, 17, 1548, 1290, 1325, 17, 67, 1103, 11344, 1325, 18, 145, 28, 746, 9, 3938, 5430, 72, 5333, 3236, 17, 63, 682, 19, 1024, 13, 5, 1320, 16, 13, 26116, 6, 2612, 15, 4123, 15944, 28, 9032, 20171, 15, 17, 14378, 14, 17074, 30, 63, 682, 19, 3151, 2612, 15, 17, 5776, 3126, 177, 72, 63, 74, 638, 29, 14, 173, 179, 1089, 17, 63, 682, 19, 13, 20240, 2612, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   guid: 1350
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 416, 19355, 13, 622, 43, 1731, 13, 5, 7917, 400, 15, 6167, 13, 10, 313, 771, 15, 5013, 6, 23, 21, 3061, 8, 381, 15, 189, 636, 2558, 6813, 6001, 1, 6813, 210, 702, 500, 500, 72, 23, 14, 64, 12802, 16, 14, 636, 2558, 2681, 1, 2408, 7005, 1, 6813, 210, 4272, 1, 22272, 1, 2681, 305, 14873, 1986, 500, 500, 636, 2558, 1694, 1850, 2187, 1, 1694, 1850, 2187, 500, 500, 9, 24, 2158, 37, 14, 636, 2558, 13809, 16, 8574, 10, 1226, 7771, 103, 500, 13, 5, 220, 9, 18, 9, 19, 6627, 6, 17, 13, 5, 79, 9, 58, 9, 19, 6183, 6, 9, 26623, 25, 40, 20535, 1597, 20, 636, 2558, 6813, 6001, 1, 6813, 6001, 500, 500, 15, 14, 949, 16, 2645, 9, 416, 19355, 13, 622, 43, 1731, 23, 386, 19, 12340, 252, 15, 636, 2558, 1385, 1911, 7597, 618, 1, 1385, 1911, 7597, 618, 500, 500, 15, 636, 2558, 29188, 1, 29188, 500, 500, 17, 12206, 20, 14, 636, 2558, 12378, 1, 3859, 18, 1, 12378, 202, 500, 500, 19, 7365, 9, 13, 3, 416, 19355, 13, 622, 43, 1731, 15, 386, 27, 313, 400, 15, 6167, 19, 12340, 252, 15, 4658, 106, 7597, 618, 15, 2692, 719, 3249, 15, 23, 21, 3156, 16, 155, 16, 8574, 10, 1226, 7771, 103, 19, 14, 575, 16, 20080, 13, 5, 124, 762, 16, 1321, 201, 17, 21, 1686, 16, 6968, 6, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   guid: 2534
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 6348, 1841, 25, 21, 399, 19, 14, 4370, 18317, 256, 16, 636, 2558, 540, 4297, 1635, 1939, 1, 540, 4297, 1635, 1939, 500, 500, 15, 636, 2558, 15357, 1, 15357, 500, 500, 9, 13, 3, 21, 399, 19, 998, 15, 19, 542, 6348, 1841, 41, 400, 4069, 148, 29, 140, 519, 13, 5, 18972, 6, 142, 2004, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   guid: 3385
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 14, 9023, 10, 11927, 2554, 528, 10377, 13, 5, 2573, 167, 28, 11083, 7161, 10377, 6, 25, 21, 1310, 636, 2558, 17345, 857, 1, 1682, 1, 108, 1, 124, 1, 20463, 1, 12378, 1, 3859, 18, 1, 17345, 857, 191, 500, 500, 424, 636, 2558, 7328, 38, 18, 4725, 15, 1, 192, 5753, 540, 1, 7328, 38, 18, 4725, 500, 500, 19, 636, 2558, 3403, 11322, 1, 16081, 15, 1, 192, 5753, 540, 1, 3403, 11322, 271, 15, 5023, 500, 500, 9, 19, 8141, 15, 21, 2275, 848, 23, 392, 15, 4721, 222, 407, 3662, 17, 21, 5062, 8, 27265, 636, 2558, 10563, 111, 1, 10563, 111, 500, 500, 9, 14, 9023, 10, 11927, 2554, 528, 10377, 13, 5, 2573, 167, 28, 11083, 7161, 10377, 6, 25, 21, 1310, 636, 2558, 17345, 857, 1, 1682, 1, 108, 1, 124, 1, 20463, 1, 12378, 1, 3859, 18, 1, 17345, 857, 191, 500, 500, 424, 636, 2558, 7328, 38, 18, 4725, 15, 1, 192, 5753, 540, 1, 7328, 38, 18, 4725, 500, 500, 19, 636, 2558, 3403, 11322, 1, 16081, 15, 1, 192, 5753, 540, 1, 3403, 11322, 271, 15, 5023, 500, 500, 9, 14, 191, 23, 392, 19, 11823, 15, 3, 14, 9023, 10, 11927, 2554, 528, 10377, 15, 21, 1310, 10377, 191, 424, 8010, 18, 4725, 19, 4670, 271, 15, 5023, 15, 63, 20301, 3386, 18854, 215, 14, 848, 16, 222, 3662, 17, 21, 21404, 19, 8141, 15, 14, 501, 1370, 13811, 169, 3454, 18, 57, 74, 10854, 29, 6497, 990, 96, 18, 9, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   guid: 1645
03/19/2023 11:23:01 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 14, 190, 13519, 35, 1768, 4725, 1374, 15, 636, 2558, 25241, 1226, 1, 10422, 1, 25241, 1226, 383, 500, 500, 9, 11108, 16528, 8, 1885, 13072, 62, 25, 21, 3903, 19, 14, 636, 2558, 10723, 790, 1, 5, 29263, 6, 1, 10723, 790, 500, 500, 636, 2558, 29263, 18, 1, 1041, 1, 10696, 1, 29263, 500, 500, 19, 636, 2558, 20561, 18, 8, 546, 8, 10696, 1, 20561, 18, 8, 546, 8, 10696, 500, 500, 19, 743, 636, 2558, 10696, 1, 10696, 500, 500, 9, 3254, 14590, 670, 14, 612, 1393, 15, 1799, 19, 14, 636, 2558, 16426, 1, 325, 3377, 1, 2642, 1, 11449, 1, 16426, 2299, 282, 365, 500, 500, 22, 18, 636, 2558, 16426, 1, 325, 3377, 1, 512, 19623, 69, 1, 1367, 1, 17557, 1, 512, 19623, 69, 349, 460, 500, 500, 28, 21, 12839, 1069, 37, 1262, 356, 4656, 163, 937, 344, 5267, 15, 131, 1647, 636, 2558, 4614, 2304, 1, 3966, 106, 8, 20293, 528, 1, 4614, 2304, 11953, 8, 20293, 528, 500, 500, 9, 13, 3, 3011, 3254, 14590, 23, 4976, 29, 132, 9830, 9173, 19, 11108, 16528, 15, 21, 3903, 19, 14, 24460, 604, 16, 743, 714, 27, 14, 453, 96, 208, 16, 327, 4156, 17, 81, 91, 114, 509, 75, 19, 327, 547, 17, 390, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/19/2023 11:23:03 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Saving features into cached file data/cached_dev_AlbertTokenizerFast_256_feverous_all [took 1.295 s]
03/19/2023 11:23:03 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/19/2023 11:23:03 - INFO - transformers.trainer -   ***** Running training *****
03/19/2023 11:23:03 - INFO - transformers.trainer -     Num examples = 29960
03/19/2023 11:23:03 - INFO - transformers.trainer -     Num Epochs = 6
03/19/2023 11:23:03 - INFO - transformers.trainer -     Instantaneous batch size per device = 8
03/19/2023 11:23:03 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8
03/19/2023 11:23:03 - INFO - transformers.trainer -     Gradient Accumulation steps = 1
03/19/2023 11:23:03 - INFO - transformers.trainer -     Total optimization steps = 20000
03/19/2023 11:32:35 - INFO - transformers.trainer -   Saving model checkpoint to results/feverous_less_steps/checkpoint-4000
03/19/2023 11:32:35 - INFO - transformers.configuration_utils -   Configuration saved in results/feverous_less_steps/checkpoint-4000/config.json
03/19/2023 11:32:36 - INFO - transformers.modeling_utils -   Model weights saved in results/feverous_less_steps/checkpoint-4000/pytorch_model.bin
03/19/2023 11:42:08 - INFO - transformers.trainer -   Saving model checkpoint to results/feverous_less_steps/checkpoint-8000
03/19/2023 11:42:08 - INFO - transformers.configuration_utils -   Configuration saved in results/feverous_less_steps/checkpoint-8000/config.json
03/19/2023 11:42:08 - INFO - transformers.modeling_utils -   Model weights saved in results/feverous_less_steps/checkpoint-8000/pytorch_model.bin
03/19/2023 11:51:41 - INFO - transformers.trainer -   Saving model checkpoint to results/feverous_less_steps/checkpoint-12000
03/19/2023 11:51:41 - INFO - transformers.configuration_utils -   Configuration saved in results/feverous_less_steps/checkpoint-12000/config.json
03/19/2023 11:51:41 - INFO - transformers.modeling_utils -   Model weights saved in results/feverous_less_steps/checkpoint-12000/pytorch_model.bin
03/19/2023 12:01:15 - INFO - transformers.trainer -   Saving model checkpoint to results/feverous_less_steps/checkpoint-16000
03/19/2023 12:01:15 - INFO - transformers.configuration_utils -   Configuration saved in results/feverous_less_steps/checkpoint-16000/config.json
03/19/2023 12:01:15 - INFO - transformers.modeling_utils -   Model weights saved in results/feverous_less_steps/checkpoint-16000/pytorch_model.bin
03/19/2023 12:10:49 - INFO - transformers.trainer -   Saving model checkpoint to results/feverous_less_steps/checkpoint-20000
03/19/2023 12:10:49 - INFO - transformers.configuration_utils -   Configuration saved in results/feverous_less_steps/checkpoint-20000/config.json
03/19/2023 12:10:49 - INFO - transformers.modeling_utils -   Model weights saved in results/feverous_less_steps/checkpoint-20000/pytorch_model.bin
03/19/2023 12:10:49 - INFO - transformers.trainer -   

Training completed. Do not forget to share your model on huggingface.co/models =)


03/19/2023 12:10:49 - INFO - transformers.trainer -   Saving model checkpoint to results/feverous_less_steps
03/19/2023 12:10:49 - INFO - transformers.configuration_utils -   Configuration saved in results/feverous_less_steps/config.json
03/19/2023 12:10:49 - INFO - transformers.modeling_utils -   Model weights saved in results/feverous_less_steps/pytorch_model.bin
03/19/2023 12:10:49 - INFO - __main__ -   *** Evaluate ***
03/19/2023 12:10:49 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/19/2023 12:10:49 - INFO - transformers.trainer -     Num examples = 3538
03/19/2023 12:10:49 - INFO - transformers.trainer -     Batch size = 128
03/19/2023 12:11:09 - INFO - __main__ -   ***** Eval results feverous *****
03/19/2023 12:11:09 - INFO - __main__ -     eval_loss = 1.3213633298873901
03/19/2023 12:11:09 - INFO - __main__ -     eval_acc = 0.7979084228377614
03/19/2023 12:11:09 - INFO - __main__ -     eval_macro_f1 = 0.6197691754532312
03/19/2023 12:11:09 - INFO - __main__ -     eval_runtime = 20.105
03/19/2023 12:11:09 - INFO - __main__ -     eval_samples_per_second = 175.977
03/19/2023 12:11:09 - INFO - __main__ -     epoch = 5.34
03/19/2023 12:11:09 - INFO - transformers.configuration_utils -   loading configuration file results/feverous_less_steps/checkpoint-4000/config.json
03/19/2023 12:11:09 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 12:11:09 - INFO - transformers.modeling_utils -   loading weights file results/feverous_less_steps/checkpoint-4000/pytorch_model.bin
03/19/2023 12:11:10 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/19/2023 12:11:10 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at results/feverous_less_steps/checkpoint-4000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/19/2023 12:11:10 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/19/2023 12:11:10 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/19/2023 12:11:10 - INFO - transformers.trainer -     Num examples = 3538
03/19/2023 12:11:10 - INFO - transformers.trainer -     Batch size = 128
03/19/2023 12:11:30 - INFO - __main__ -   ***** Eval results feverous: results/feverous_less_steps/checkpoint-4000 *****
03/19/2023 12:11:30 - INFO - __main__ -     eval_loss = 0.7643173336982727
03/19/2023 12:11:30 - INFO - __main__ -     eval_acc = 0.8182589033352177
03/19/2023 12:11:30 - INFO - __main__ -     eval_macro_f1 = 0.5694798298802127
03/19/2023 12:11:30 - INFO - __main__ -     eval_runtime = 20.2879
03/19/2023 12:11:30 - INFO - __main__ -     eval_samples_per_second = 174.39
03/19/2023 12:11:30 - INFO - transformers.configuration_utils -   loading configuration file results/feverous_less_steps/checkpoint-8000/config.json
03/19/2023 12:11:30 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 12:11:30 - INFO - transformers.modeling_utils -   loading weights file results/feverous_less_steps/checkpoint-8000/pytorch_model.bin
03/19/2023 12:11:31 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/19/2023 12:11:31 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at results/feverous_less_steps/checkpoint-8000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/19/2023 12:11:31 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/19/2023 12:11:31 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/19/2023 12:11:31 - INFO - transformers.trainer -     Num examples = 3538
03/19/2023 12:11:31 - INFO - transformers.trainer -     Batch size = 128
03/19/2023 12:11:51 - INFO - __main__ -   ***** Eval results feverous: results/feverous_less_steps/checkpoint-8000 *****
03/19/2023 12:11:51 - INFO - __main__ -     eval_loss = 0.9651147127151489
03/19/2023 12:11:51 - INFO - __main__ -     eval_acc = 0.8109101187111363
03/19/2023 12:11:51 - INFO - __main__ -     eval_macro_f1 = 0.5665550155204441
03/19/2023 12:11:51 - INFO - __main__ -     eval_runtime = 20.2226
03/19/2023 12:11:51 - INFO - __main__ -     eval_samples_per_second = 174.953
03/19/2023 12:11:51 - INFO - transformers.configuration_utils -   loading configuration file results/feverous_less_steps/checkpoint-12000/config.json
03/19/2023 12:11:51 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 12:11:51 - INFO - transformers.modeling_utils -   loading weights file results/feverous_less_steps/checkpoint-12000/pytorch_model.bin
03/19/2023 12:11:52 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/19/2023 12:11:52 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at results/feverous_less_steps/checkpoint-12000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/19/2023 12:11:52 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/19/2023 12:11:52 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/19/2023 12:11:52 - INFO - transformers.trainer -     Num examples = 3538
03/19/2023 12:11:52 - INFO - transformers.trainer -     Batch size = 128
03/19/2023 12:12:12 - INFO - __main__ -   ***** Eval results feverous: results/feverous_less_steps/checkpoint-12000 *****
03/19/2023 12:12:12 - INFO - __main__ -     eval_loss = 0.9882345795631409
03/19/2023 12:12:12 - INFO - __main__ -     eval_acc = 0.8078010175240249
03/19/2023 12:12:12 - INFO - __main__ -     eval_macro_f1 = 0.6065232006151283
03/19/2023 12:12:12 - INFO - __main__ -     eval_runtime = 20.303
03/19/2023 12:12:12 - INFO - __main__ -     eval_samples_per_second = 174.26
03/19/2023 12:12:12 - INFO - transformers.configuration_utils -   loading configuration file results/feverous_less_steps/checkpoint-16000/config.json
03/19/2023 12:12:12 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 12:12:12 - INFO - transformers.modeling_utils -   loading weights file results/feverous_less_steps/checkpoint-16000/pytorch_model.bin
03/19/2023 12:12:13 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/19/2023 12:12:13 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at results/feverous_less_steps/checkpoint-16000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/19/2023 12:12:13 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/19/2023 12:12:13 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/19/2023 12:12:13 - INFO - transformers.trainer -     Num examples = 3538
03/19/2023 12:12:13 - INFO - transformers.trainer -     Batch size = 128
03/19/2023 12:12:33 - INFO - __main__ -   ***** Eval results feverous: results/feverous_less_steps/checkpoint-16000 *****
03/19/2023 12:12:33 - INFO - __main__ -     eval_loss = 1.1609175205230713
03/19/2023 12:12:33 - INFO - __main__ -     eval_acc = 0.8044092707744488
03/19/2023 12:12:33 - INFO - __main__ -     eval_macro_f1 = 0.6184082442823738
03/19/2023 12:12:33 - INFO - __main__ -     eval_runtime = 20.2383
03/19/2023 12:12:33 - INFO - __main__ -     eval_samples_per_second = 174.817
03/19/2023 12:12:33 - INFO - transformers.configuration_utils -   loading configuration file results/feverous_less_steps/checkpoint-20000/config.json
03/19/2023 12:12:33 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 12:12:33 - INFO - transformers.modeling_utils -   loading weights file results/feverous_less_steps/checkpoint-20000/pytorch_model.bin
03/19/2023 12:12:34 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/19/2023 12:12:34 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at results/feverous_less_steps/checkpoint-20000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/19/2023 12:12:34 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/19/2023 12:12:34 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/19/2023 12:12:34 - INFO - transformers.trainer -     Num examples = 3538
03/19/2023 12:12:34 - INFO - transformers.trainer -     Batch size = 128
03/19/2023 12:12:54 - INFO - __main__ -   ***** Eval results feverous: results/feverous_less_steps/checkpoint-20000 *****
03/19/2023 12:12:54 - INFO - __main__ -     eval_loss = 1.3213633298873901
03/19/2023 12:12:54 - INFO - __main__ -     eval_acc = 0.7979084228377614
03/19/2023 12:12:54 - INFO - __main__ -     eval_macro_f1 = 0.6197691754532312
03/19/2023 12:12:54 - INFO - __main__ -     eval_runtime = 20.1766
03/19/2023 12:12:54 - INFO - __main__ -     eval_samples_per_second = 175.351
03/19/2023 12:12:54 - INFO - __main__ -   ***** Best eval accuracy: 0.8182589033352177, 'results/feverous_less_steps/checkpoint-4000' *****
03/19/2023 12:12:54 - INFO - transformers.configuration_utils -   loading configuration file results/feverous_less_steps/checkpoint-4000/config.json
03/19/2023 12:12:54 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 12:12:54 - INFO - transformers.modeling_utils -   loading weights file results/feverous_less_steps/checkpoint-4000/pytorch_model.bin
03/19/2023 12:12:54 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/19/2023 12:12:54 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at results/feverous_less_steps/checkpoint-4000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/19/2023 12:12:54 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/19/2023 12:12:54 - INFO - __main__ -   Loading best model from results/feverous_less_steps/checkpoint-4000
03/19/2023 12:12:54 - INFO - transformers.configuration_utils -   loading configuration file results/feverous_less_steps/checkpoint-4000/config.json
03/19/2023 12:12:54 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/19/2023 12:12:54 - INFO - transformers.modeling_utils -   loading weights file results/feverous_less_steps/checkpoint-4000/pytorch_model.bin
03/19/2023 12:12:55 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/19/2023 12:12:55 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at results/feverous_less_steps/checkpoint-4000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/19/2023 12:12:55 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Collecting test examples (ratio=1.0) from dataset file at data/feverous
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   guid: 5ed4de07c9e77c000848a180_1
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 496, 20, 14, 388, 29867, 596, 1230, 13, 5, 334, 2159, 13, 6, 13, 15, 280, 4102, 63, 74, 6935, 26, 539, 507, 2675, 17, 561, 9, 457, 507, 2391, 13, 15, 29, 21, 600, 16, 91, 119, 1137, 507, 2461, 2598, 19, 14, 1063, 13, 9, 3, 280, 4102, 117, 131, 1137, 9, 264, 507, 2598, 19, 14, 1063, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   guid: 5ed4de07c9e77c000848a180_2
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 496, 20, 14, 388, 29867, 596, 1230, 13, 5, 334, 2159, 13, 6, 13, 15, 280, 4102, 63, 74, 6935, 26, 539, 507, 2675, 13, 15, 137, 9, 240, 507, 763, 2675, 13, 15, 17, 561, 9, 457, 507, 2391, 13, 15, 29, 21, 600, 16, 91, 119, 937, 507, 2461, 2598, 19, 14, 1063, 13, 9, 3, 280, 4102, 117, 131, 1137, 9, 264, 507, 2598, 19, 14, 1063, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   guid: 5ed4de07c9e77c000848a180_3
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 496, 20, 14, 388, 29867, 596, 1230, 13, 5, 334, 2159, 13, 6, 13, 15, 280, 4102, 63, 74, 6935, 26, 539, 507, 2675, 17, 561, 9, 457, 507, 2391, 13, 15, 29, 21, 600, 16, 91, 119, 1137, 507, 2461, 2598, 19, 14, 1063, 13, 9, 3, 280, 4102, 945, 91, 119, 137, 507, 763, 2675, 17, 117, 84, 1137, 9, 264, 507, 2598, 19, 14, 1063, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   guid: 5ed4de07c9e77c000848a180_4
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 496, 20, 14, 388, 29867, 596, 1230, 13, 5, 334, 2159, 13, 6, 13, 15, 280, 4102, 63, 74, 6935, 26, 539, 507, 2675, 13, 15, 137, 9, 240, 507, 763, 2675, 13, 15, 17, 561, 9, 457, 507, 2391, 13, 15, 29, 21, 600, 16, 91, 119, 937, 507, 2461, 2598, 19, 14, 1063, 13, 9, 3, 280, 4102, 945, 91, 119, 137, 507, 763, 2675, 17, 117, 84, 1137, 9, 264, 507, 2598, 19, 14, 1063, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   *** Example ***
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   guid: 5ee39329c9e77c0008cc93d1_1
03/19/2023 12:13:11 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 28, 21, 20474, 1857, 4058, 14, 283, 63, 7485, 5822, 18, 16, 91, 119, 808, 148, 17, 14, 2792, 16, 148, 37, 1166, 7997, 183, 8, 20989, 9, 3, 1180, 1166, 57, 11875, 69, 5822, 18, 16, 91, 119, 808, 148, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/19/2023 12:13:32 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Saving features into cached file data/cached_test_AlbertTokenizerFast_256_feverous_all [took 19.766 s]
03/19/2023 12:13:32 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/19/2023 12:13:32 - INFO - __main__ -   Evaluating on feverous
03/19/2023 12:13:32 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/19/2023 12:13:32 - INFO - transformers.trainer -     Num examples = 55197
03/19/2023 12:13:32 - INFO - transformers.trainer -     Batch size = 128
03/19/2023 12:18:47 - INFO - __main__ -   ***** Test results feverous *****
03/19/2023 12:18:47 - INFO - __main__ -     eval_loss = 1.3992326259613037
03/19/2023 12:18:47 - INFO - __main__ -     eval_acc = 0.6790042937116147
03/19/2023 12:18:47 - INFO - __main__ -     eval_macro_f1 = 0.48500915441790643
03/19/2023 12:18:47 - INFO - __main__ -     eval_runtime = 314.8115
03/19/2023 12:18:47 - INFO - __main__ -     eval_samples_per_second = 175.334
