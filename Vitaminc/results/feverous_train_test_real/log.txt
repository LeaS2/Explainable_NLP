03/28/2023 13:10:47 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
03/28/2023 13:10:47 - INFO - __main__ -   Training/evaluation parameters VitCTrainingArgs(output_dir='../../../../netscratch/bleick/feverous_train_test_real', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=128, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=20000, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Mar28_13-10-46_serv-9208', logging_first_step=False, logging_steps=500, save_steps=4000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../../netscratch/bleick/feverous_train_test_real', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, eval_all_checkpoints=True, do_test=True, test_on_best_ckpt=True)
03/28/2023 13:10:47 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpq7qn_iac
03/28/2023 13:10:47 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/41fd59466159d48786110897b3d780f83acfbc5bc8282ab17cf888a3aecbef49.8b3d8d4ae5da2b27fc470a9914f3e870cf8a87f573b71c603ce66be72f09abe7
03/28/2023 13:10:47 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/41fd59466159d48786110897b3d780f83acfbc5bc8282ab17cf888a3aecbef49.8b3d8d4ae5da2b27fc470a9914f3e870cf8a87f573b71c603ce66be72f09abe7
03/28/2023 13:10:47 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/41fd59466159d48786110897b3d780f83acfbc5bc8282ab17cf888a3aecbef49.8b3d8d4ae5da2b27fc470a9914f3e870cf8a87f573b71c603ce66be72f09abe7
03/28/2023 13:10:47 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "vitc_fever/checkpoint-50000",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 13:10:48 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/41fd59466159d48786110897b3d780f83acfbc5bc8282ab17cf888a3aecbef49.8b3d8d4ae5da2b27fc470a9914f3e870cf8a87f573b71c603ce66be72f09abe7
03/28/2023 13:10:48 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "vitc_fever/checkpoint-50000",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "vitc_real",
    "vitc_synthetic",
    "fever"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "SUPPORTS",
    "1": "REFUTES",
    "2": "NOT ENOUGH INFO"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "NOT ENOUGH INFO": "2",
    "REFUTES": "1",
    "SUPPORTS": "0"
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 13:10:48 - INFO - transformers.tokenization_utils_base -   Model name 'tals/albert-base-vitaminc-fever' not found in model shortcut name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). Assuming 'tals/albert-base-vitaminc-fever' is a path, a model identifier, or url to a directory containing tokenizer files.
03/28/2023 13:10:48 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu2xu4qaj
03/28/2023 13:10:49 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/7aacaf1e4ae6e087e966a814e6071a2bf1c80be24f4c0cc57fa0907242ea68e4.1f8b90a03f83ee30ea32a414a46d417a91d5ed797e90664b02b7591531a8e0e9
03/28/2023 13:10:49 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/7aacaf1e4ae6e087e966a814e6071a2bf1c80be24f4c0cc57fa0907242ea68e4.1f8b90a03f83ee30ea32a414a46d417a91d5ed797e90664b02b7591531a8e0e9
03/28/2023 13:10:50 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpy8y3j94w
03/28/2023 13:10:51 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/8c82e8d7c3bcf544d301b8d4777d1b6dd7b700c7f58c955f19d289a1f70eceeb.623993453f3f6b9f6ad831899812f482e5cde100e664124feb3a6446d69a26bf
03/28/2023 13:10:51 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/8c82e8d7c3bcf544d301b8d4777d1b6dd7b700c7f58c955f19d289a1f70eceeb.623993453f3f6b9f6ad831899812f482e5cde100e664124feb3a6446d69a26bf
03/28/2023 13:10:51 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj6ihw0hu
03/28/2023 13:10:52 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/3a40dcef47d1d3a91cda05c752bbcd6206d3b3f9d51c0de439adc38b652da49d.7f7ace06b0faaa16d30d7aba993112e16d81588fd5afa15b88262da4af208014
03/28/2023 13:10:52 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/3a40dcef47d1d3a91cda05c752bbcd6206d3b3f9d51c0de439adc38b652da49d.7f7ace06b0faaa16d30d7aba993112e16d81588fd5afa15b88262da4af208014
03/28/2023 13:10:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/7aacaf1e4ae6e087e966a814e6071a2bf1c80be24f4c0cc57fa0907242ea68e4.1f8b90a03f83ee30ea32a414a46d417a91d5ed797e90664b02b7591531a8e0e9
03/28/2023 13:10:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/tokenizer.json from cache at None
03/28/2023 13:10:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/added_tokens.json from cache at None
03/28/2023 13:10:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/8c82e8d7c3bcf544d301b8d4777d1b6dd7b700c7f58c955f19d289a1f70eceeb.623993453f3f6b9f6ad831899812f482e5cde100e664124feb3a6446d69a26bf
03/28/2023 13:10:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/3a40dcef47d1d3a91cda05c752bbcd6206d3b3f9d51c0de439adc38b652da49d.7f7ace06b0faaa16d30d7aba993112e16d81588fd5afa15b88262da4af208014
03/28/2023 13:10:52 - INFO - transformers.file_utils -   https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqul0s5c4
03/28/2023 13:10:55 - INFO - transformers.file_utils -   storing https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bc146aa70c6b970fc726f903e34f2b06b914886a365409e34b9fd738807ecfc7.36d94c7865a57ac31a6ac22b015e6400e842e3bec8b7ab9a68a001ee2ac8a367
03/28/2023 13:10:55 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/bc146aa70c6b970fc726f903e34f2b06b914886a365409e34b9fd738807ecfc7.36d94c7865a57ac31a6ac22b015e6400e842e3bec8b7ab9a68a001ee2ac8a367
03/28/2023 13:10:55 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/tals/albert-base-vitaminc-fever/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bc146aa70c6b970fc726f903e34f2b06b914886a365409e34b9fd738807ecfc7.36d94c7865a57ac31a6ac22b015e6400e842e3bec8b7ab9a68a001ee2ac8a367
03/28/2023 13:10:55 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/28/2023 13:10:55 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at tals/albert-base-vitaminc-fever.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/28/2023 13:10:55 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Collecting train examples (ratio=1.0) from dataset file at data/feverous
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   guid: 60021
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 6438, 15, 398, 6258, 22, 18, 485, 891, 6747, 14, 5769, 16, 636, 2558, 242, 106, 3366, 1, 5909, 1807, 43, 1, 242, 106, 3366, 15935, 500, 500, 1275, 102, 9, 398, 6258, 3767, 50, 4173, 8, 23039, 19, 14, 807, 16, 1940, 2990, 17, 555, 1890, 8, 13120, 17220, 18, 9, 398, 6258, 630, 52, 1181, 82, 258, 1054, 8, 18, 2153, 1303, 329, 15, 47, 1927, 1054, 8, 12097, 17, 6046, 17991, 18, 20, 1242, 145, 28, 636, 2558, 3812, 6931, 1, 14113, 1, 11449, 18, 5910, 19293, 18, 1, 3812, 6931, 6399, 17729, 500, 500, 15, 636, 2558, 7738, 2569, 1, 657, 410, 657, 1, 7738, 2569, 1332, 410, 657, 500, 500, 15, 636, 2558, 192, 9452, 1963, 1, 192, 9452, 1963, 500, 500, 15, 636, 2558, 252, 8736, 18, 1, 252, 8736, 18, 500, 500, 15, 17140, 17825, 15, 17, 931, 9428, 25136, 9, 13, 3, 17140, 398, 6258, 1103, 21, 13613, 4173, 15220, 6534, 432, 27, 4387, 23306, 18, 15, 17, 19, 14, 610, 16, 21, 2990, 15, 21, 398, 6258, 625, 29, 23306, 68, 9338, 129, 15, 685, 6550, 15, 13113, 5511, 37, 14, 236, 1066, 23306, 15, 17, 398, 6258, 67, 1103, 21, 6534, 227, 2079, 3132, 18, 15, 56, 50, 23671, 8, 3367, 11356, 69, 23306, 18, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   guid: 40697
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 19, 13250, 15, 40, 384, 1436, 296, 97, 20, 648, 29, 53, 16, 33, 3945, 73, 27, 33, 788, 20, 13789, 15, 24, 23, 23808, 69, 19, 14, 636, 2558, 43, 22, 2291, 99, 6146, 14693, 1, 10928, 1, 43, 22, 2291, 99, 6146, 14693, 1318, 500, 500, 9, 19, 11823, 24, 199, 6485, 20, 636, 2558, 1993, 1, 9211, 6013, 1, 1993, 10634, 500, 500, 15, 945, 105, 20, 239, 17, 28030, 25386, 6444, 15, 17, 587, 20, 635, 721, 6493, 15, 33, 321, 22, 18, 2028, 9, 13, 3, 19, 13250, 15, 665, 2009, 40, 384, 1436, 296, 97, 20, 648, 29, 53, 16, 33, 3945, 73, 27, 33, 788, 20, 13789, 15, 24, 23, 23808, 69, 19, 14, 13, 43, 22, 2291, 99, 6146, 14693, 1318, 17, 19, 11823, 24, 199, 6485, 20, 1295, 10634, 15, 945, 105, 20, 239, 17, 28030, 25386, 6444, 15, 17, 587, 20, 635, 721, 6493, 15, 33, 321, 22, 18, 2028, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   guid: 61669
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 14, 650, 17410, 22, 213, 636, 2558, 8686, 103, 4228, 1, 5, 22867, 1, 16154, 18, 1, 27071, 6, 1, 8686, 103, 4228, 500, 500, 967, 81, 2318, 10095, 19155, 18, 1, 849, 402, 17, 53, 359, 1, 218, 21, 612, 705, 2395, 9, 14, 690, 12166, 920, 426, 23, 40, 636, 2558, 22867, 1, 16154, 18, 1, 27071, 1, 22867, 1761, 435, 500, 500, 730, 6499, 128, 14, 636, 2558, 4980, 43, 2186, 1, 18, 3019, 18, 1, 4980, 43, 2186, 6494, 18, 500, 500, 17, 14, 636, 2558, 6514, 1, 14374, 8462, 18, 1, 6514, 17410, 500, 500, 35, 14, 636, 2558, 4240, 8095, 1, 150, 4869, 1198, 1, 8810, 1, 4240, 8095, 2728, 666, 500, 500, 27, 137, 311, 690, 9, 13, 3, 14, 650, 17410, 144, 52, 3705, 19, 14, 690, 198, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   guid: 72963
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 33, 360, 14, 2580, 16, 14, 1819, 3074, 25, 53, 16, 14, 246, 8, 10033, 3037, 37, 14, 155, 16, 1033, 901, 9, 13, 3, 6754, 1619, 23, 1314, 16, 21, 246, 4355, 360, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   guid: 451
03/28/2023 13:11:08 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 14, 599, 23, 230, 34, 4913, 13, 5, 3216, 819, 6, 72, 1800, 3249, 13, 5, 4268, 819, 6, 17, 636, 2558, 24476, 1, 6919, 1, 6708, 1443, 1, 6575, 1, 24476, 500, 500, 13, 5, 3555, 819, 6, 9, 13, 3, 35, 14, 126, 426, 16, 14, 1752, 11317, 126, 7473, 599, 15, 14, 3419, 4800, 15460, 17, 8115, 1223, 43, 9050, 104, 1344, 1417, 819, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/28/2023 13:11:17 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Saving features into cached file data/cached_train_AlbertTokenizerFast_256_feverous_all [took 8.357 s]
03/28/2023 13:11:17 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Collecting dev examples (ratio=1.0) from dataset file at data/feverous
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   guid: 3044
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 190, 1244, 25, 40, 189, 5784, 13435, 679, 34, 636, 2558, 18, 6268, 1, 6893, 3700, 7437, 1, 18, 6268, 1572, 3700, 7437, 500, 500, 26, 14, 636, 2558, 18219, 1, 24162, 68, 1, 28359, 1, 18219, 3647, 237, 500, 500, 30, 4850, 27, 320, 1538, 15, 1247, 9, 179, 82, 5421, 15, 190, 1244, 63, 74, 2525, 10410, 9, 13, 3, 190, 1244, 25, 40, 189, 5784, 13435, 30, 967, 355, 407, 430, 4977, 15, 17, 1548, 1290, 1325, 17, 67, 1103, 11344, 1325, 18, 145, 28, 746, 9, 3938, 5430, 72, 5333, 3236, 17, 63, 682, 19, 1024, 13, 5, 1320, 16, 13, 26116, 6, 2612, 15, 4123, 15944, 28, 9032, 20171, 15, 17, 14378, 14, 17074, 30, 63, 682, 19, 3151, 2612, 15, 17, 5776, 3126, 177, 72, 63, 74, 638, 29, 14, 173, 179, 1089, 17, 63, 682, 19, 13, 20240, 2612, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   guid: 1350
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 416, 19355, 13, 622, 43, 1731, 13, 5, 7917, 400, 15, 6167, 13, 10, 313, 771, 15, 5013, 6, 23, 21, 3061, 8, 381, 15, 189, 636, 2558, 6813, 6001, 1, 6813, 210, 702, 500, 500, 72, 23, 14, 64, 12802, 16, 14, 636, 2558, 2681, 1, 2408, 7005, 1, 6813, 210, 4272, 1, 22272, 1, 2681, 305, 14873, 1986, 500, 500, 636, 2558, 1694, 1850, 2187, 1, 1694, 1850, 2187, 500, 500, 9, 24, 2158, 37, 14, 636, 2558, 13809, 16, 8574, 10, 1226, 7771, 103, 500, 13, 5, 220, 9, 18, 9, 19, 6627, 6, 17, 13, 5, 79, 9, 58, 9, 19, 6183, 6, 9, 26623, 25, 40, 20535, 1597, 20, 636, 2558, 6813, 6001, 1, 6813, 6001, 500, 500, 15, 14, 949, 16, 2645, 9, 416, 19355, 13, 622, 43, 1731, 23, 386, 19, 12340, 252, 15, 636, 2558, 1385, 1911, 7597, 618, 1, 1385, 1911, 7597, 618, 500, 500, 15, 636, 2558, 29188, 1, 29188, 500, 500, 17, 12206, 20, 14, 636, 2558, 12378, 1, 3859, 18, 1, 12378, 202, 500, 500, 19, 7365, 9, 13, 3, 416, 19355, 13, 622, 43, 1731, 15, 386, 27, 313, 400, 15, 6167, 19, 12340, 252, 15, 4658, 106, 7597, 618, 15, 2692, 719, 3249, 15, 23, 21, 3156, 16, 155, 16, 8574, 10, 1226, 7771, 103, 19, 14, 575, 16, 20080, 13, 5, 124, 762, 16, 1321, 201, 17, 21, 1686, 16, 6968, 6, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   guid: 2534
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 6348, 1841, 25, 21, 399, 19, 14, 4370, 18317, 256, 16, 636, 2558, 540, 4297, 1635, 1939, 1, 540, 4297, 1635, 1939, 500, 500, 15, 636, 2558, 15357, 1, 15357, 500, 500, 9, 13, 3, 21, 399, 19, 998, 15, 19, 542, 6348, 1841, 41, 400, 4069, 148, 29, 140, 519, 13, 5, 18972, 6, 142, 2004, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   guid: 3385
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 14, 9023, 10, 11927, 2554, 528, 10377, 13, 5, 2573, 167, 28, 11083, 7161, 10377, 6, 25, 21, 1310, 636, 2558, 17345, 857, 1, 1682, 1, 108, 1, 124, 1, 20463, 1, 12378, 1, 3859, 18, 1, 17345, 857, 191, 500, 500, 424, 636, 2558, 7328, 38, 18, 4725, 15, 1, 192, 5753, 540, 1, 7328, 38, 18, 4725, 500, 500, 19, 636, 2558, 3403, 11322, 1, 16081, 15, 1, 192, 5753, 540, 1, 3403, 11322, 271, 15, 5023, 500, 500, 9, 19, 8141, 15, 21, 2275, 848, 23, 392, 15, 4721, 222, 407, 3662, 17, 21, 5062, 8, 27265, 636, 2558, 10563, 111, 1, 10563, 111, 500, 500, 9, 14, 9023, 10, 11927, 2554, 528, 10377, 13, 5, 2573, 167, 28, 11083, 7161, 10377, 6, 25, 21, 1310, 636, 2558, 17345, 857, 1, 1682, 1, 108, 1, 124, 1, 20463, 1, 12378, 1, 3859, 18, 1, 17345, 857, 191, 500, 500, 424, 636, 2558, 7328, 38, 18, 4725, 15, 1, 192, 5753, 540, 1, 7328, 38, 18, 4725, 500, 500, 19, 636, 2558, 3403, 11322, 1, 16081, 15, 1, 192, 5753, 540, 1, 3403, 11322, 271, 15, 5023, 500, 500, 9, 14, 191, 23, 392, 19, 11823, 15, 3, 14, 9023, 10, 11927, 2554, 528, 10377, 15, 21, 1310, 10377, 191, 424, 8010, 18, 4725, 19, 4670, 271, 15, 5023, 15, 63, 20301, 3386, 18854, 215, 14, 848, 16, 222, 3662, 17, 21, 21404, 19, 8141, 15, 14, 501, 1370, 13811, 169, 3454, 18, 57, 74, 10854, 29, 6497, 990, 96, 18, 9, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   guid: 1645
03/28/2023 13:11:19 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 14, 190, 13519, 35, 1768, 4725, 1374, 15, 636, 2558, 25241, 1226, 1, 10422, 1, 25241, 1226, 383, 500, 500, 9, 11108, 16528, 8, 1885, 13072, 62, 25, 21, 3903, 19, 14, 636, 2558, 10723, 790, 1, 5, 29263, 6, 1, 10723, 790, 500, 500, 636, 2558, 29263, 18, 1, 1041, 1, 10696, 1, 29263, 500, 500, 19, 636, 2558, 20561, 18, 8, 546, 8, 10696, 1, 20561, 18, 8, 546, 8, 10696, 500, 500, 19, 743, 636, 2558, 10696, 1, 10696, 500, 500, 9, 3254, 14590, 670, 14, 612, 1393, 15, 1799, 19, 14, 636, 2558, 16426, 1, 325, 3377, 1, 2642, 1, 11449, 1, 16426, 2299, 282, 365, 500, 500, 22, 18, 636, 2558, 16426, 1, 325, 3377, 1, 512, 19623, 69, 1, 1367, 1, 17557, 1, 512, 19623, 69, 349, 460, 500, 500, 28, 21, 12839, 1069, 37, 1262, 356, 4656, 163, 937, 344, 5267, 15, 131, 1647, 636, 2558, 4614, 2304, 1, 3966, 106, 8, 20293, 528, 1, 4614, 2304, 11953, 8, 20293, 528, 500, 500, 9, 13, 3, 3011, 3254, 14590, 23, 4976, 29, 132, 9830, 9173, 19, 11108, 16528, 15, 21, 3903, 19, 14, 24460, 604, 16, 743, 714, 27, 14, 453, 96, 208, 16, 327, 4156, 17, 81, 91, 114, 509, 75, 19, 327, 547, 17, 390, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/28/2023 13:11:20 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Saving features into cached file data/cached_dev_AlbertTokenizerFast_256_feverous_all [took 1.254 s]
03/28/2023 13:11:20 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/28/2023 13:11:20 - INFO - transformers.trainer -   ***** Running training *****
03/28/2023 13:11:20 - INFO - transformers.trainer -     Num examples = 23968
03/28/2023 13:11:20 - INFO - transformers.trainer -     Num Epochs = 27
03/28/2023 13:11:20 - INFO - transformers.trainer -     Instantaneous batch size per device = 32
03/28/2023 13:11:20 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 32
03/28/2023 13:11:20 - INFO - transformers.trainer -     Gradient Accumulation steps = 1
03/28/2023 13:11:20 - INFO - transformers.trainer -     Total optimization steps = 20000
03/28/2023 13:48:11 - INFO - transformers.trainer -   Saving model checkpoint to ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-4000
03/28/2023 13:48:11 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-4000/config.json
03/28/2023 13:48:11 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-4000/pytorch_model.bin
03/28/2023 14:25:01 - INFO - transformers.trainer -   Saving model checkpoint to ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-8000
03/28/2023 14:25:01 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-8000/config.json
03/28/2023 14:25:01 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-8000/pytorch_model.bin
03/28/2023 15:01:52 - INFO - transformers.trainer -   Saving model checkpoint to ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-12000
03/28/2023 15:01:52 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-12000/config.json
03/28/2023 15:01:52 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-12000/pytorch_model.bin
03/28/2023 15:38:44 - INFO - transformers.trainer -   Saving model checkpoint to ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000
03/28/2023 15:38:44 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000/config.json
03/28/2023 15:38:44 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000/pytorch_model.bin
03/28/2023 16:15:33 - INFO - transformers.trainer -   Saving model checkpoint to ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-20000
03/28/2023 16:15:33 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-20000/config.json
03/28/2023 16:15:33 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-20000/pytorch_model.bin
03/28/2023 16:15:33 - INFO - transformers.trainer -   

Training completed. Do not forget to share your model on huggingface.co/models =)


03/28/2023 16:15:33 - INFO - transformers.trainer -   Saving model checkpoint to ../../../../netscratch/bleick/feverous_train_test_real
03/28/2023 16:15:33 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../netscratch/bleick/feverous_train_test_real/config.json
03/28/2023 16:15:33 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../netscratch/bleick/feverous_train_test_real/pytorch_model.bin
03/28/2023 16:15:33 - INFO - __main__ -   *** Evaluate ***
03/28/2023 16:15:33 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/28/2023 16:15:33 - INFO - transformers.trainer -     Num examples = 3538
03/28/2023 16:15:33 - INFO - transformers.trainer -     Batch size = 128
03/28/2023 16:15:55 - INFO - __main__ -   ***** Eval results feverous *****
03/28/2023 16:15:55 - INFO - __main__ -     eval_loss = 2.051514148712158
03/28/2023 16:15:55 - INFO - __main__ -     eval_acc = 0.8103448275862069
03/28/2023 16:15:55 - INFO - __main__ -     eval_macro_f1 = 0.6094969025752005
03/28/2023 16:15:55 - INFO - __main__ -     eval_runtime = 21.3974
03/28/2023 16:15:55 - INFO - __main__ -     eval_samples_per_second = 165.347
03/28/2023 16:15:55 - INFO - __main__ -     epoch = 26.7
03/28/2023 16:15:55 - INFO - transformers.configuration_utils -   loading configuration file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-4000/config.json
03/28/2023 16:15:55 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 16:15:55 - INFO - transformers.modeling_utils -   loading weights file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-4000/pytorch_model.bin
03/28/2023 16:15:56 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/28/2023 16:15:56 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-4000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/28/2023 16:15:56 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/28/2023 16:15:56 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/28/2023 16:15:56 - INFO - transformers.trainer -     Num examples = 3538
03/28/2023 16:15:56 - INFO - transformers.trainer -     Batch size = 128
03/28/2023 16:16:17 - INFO - __main__ -   ***** Eval results feverous: ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-4000 *****
03/28/2023 16:16:17 - INFO - __main__ -     eval_loss = 1.2258144617080688
03/28/2023 16:16:17 - INFO - __main__ -     eval_acc = 0.814584511023177
03/28/2023 16:16:17 - INFO - __main__ -     eval_macro_f1 = 0.6002604365617491
03/28/2023 16:16:17 - INFO - __main__ -     eval_runtime = 21.2187
03/28/2023 16:16:17 - INFO - __main__ -     eval_samples_per_second = 166.74
03/28/2023 16:16:17 - INFO - transformers.configuration_utils -   loading configuration file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-8000/config.json
03/28/2023 16:16:17 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 16:16:17 - INFO - transformers.modeling_utils -   loading weights file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-8000/pytorch_model.bin
03/28/2023 16:16:18 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/28/2023 16:16:18 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-8000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/28/2023 16:16:18 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/28/2023 16:16:18 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/28/2023 16:16:18 - INFO - transformers.trainer -     Num examples = 3538
03/28/2023 16:16:18 - INFO - transformers.trainer -     Batch size = 128
03/28/2023 16:16:39 - INFO - __main__ -   ***** Eval results feverous: ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-8000 *****
03/28/2023 16:16:39 - INFO - __main__ -     eval_loss = 1.727442741394043
03/28/2023 16:16:39 - INFO - __main__ -     eval_acc = 0.8058224985867722
03/28/2023 16:16:39 - INFO - __main__ -     eval_macro_f1 = 0.6117937133877637
03/28/2023 16:16:39 - INFO - __main__ -     eval_runtime = 21.1465
03/28/2023 16:16:39 - INFO - __main__ -     eval_samples_per_second = 167.309
03/28/2023 16:16:39 - INFO - transformers.configuration_utils -   loading configuration file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-12000/config.json
03/28/2023 16:16:39 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 16:16:39 - INFO - transformers.modeling_utils -   loading weights file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-12000/pytorch_model.bin
03/28/2023 16:16:40 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/28/2023 16:16:40 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-12000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/28/2023 16:16:40 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/28/2023 16:16:40 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/28/2023 16:16:40 - INFO - transformers.trainer -     Num examples = 3538
03/28/2023 16:16:40 - INFO - transformers.trainer -     Batch size = 128
03/28/2023 16:17:02 - INFO - __main__ -   ***** Eval results feverous: ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-12000 *****
03/28/2023 16:17:02 - INFO - __main__ -     eval_loss = 1.8489949703216553
03/28/2023 16:17:02 - INFO - __main__ -     eval_acc = 0.8072357263990956
03/28/2023 16:17:02 - INFO - __main__ -     eval_macro_f1 = 0.5995945601998341
03/28/2023 16:17:02 - INFO - __main__ -     eval_runtime = 21.1372
03/28/2023 16:17:02 - INFO - __main__ -     eval_samples_per_second = 167.383
03/28/2023 16:17:02 - INFO - transformers.configuration_utils -   loading configuration file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000/config.json
03/28/2023 16:17:02 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 16:17:02 - INFO - transformers.modeling_utils -   loading weights file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000/pytorch_model.bin
03/28/2023 16:17:03 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/28/2023 16:17:03 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/28/2023 16:17:03 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/28/2023 16:17:03 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/28/2023 16:17:03 - INFO - transformers.trainer -     Num examples = 3538
03/28/2023 16:17:03 - INFO - transformers.trainer -     Batch size = 128
03/28/2023 16:17:24 - INFO - __main__ -   ***** Eval results feverous: ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000 *****
03/28/2023 16:17:24 - INFO - __main__ -     eval_loss = 1.9445785284042358
03/28/2023 16:17:24 - INFO - __main__ -     eval_acc = 0.8157150932730356
03/28/2023 16:17:24 - INFO - __main__ -     eval_macro_f1 = 0.6075926660393768
03/28/2023 16:17:24 - INFO - __main__ -     eval_runtime = 21.1673
03/28/2023 16:17:24 - INFO - __main__ -     eval_samples_per_second = 167.145
03/28/2023 16:17:24 - INFO - transformers.configuration_utils -   loading configuration file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-20000/config.json
03/28/2023 16:17:24 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 16:17:24 - INFO - transformers.modeling_utils -   loading weights file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-20000/pytorch_model.bin
03/28/2023 16:17:25 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/28/2023 16:17:25 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-20000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/28/2023 16:17:25 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/28/2023 16:17:25 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/28/2023 16:17:25 - INFO - transformers.trainer -     Num examples = 3538
03/28/2023 16:17:25 - INFO - transformers.trainer -     Batch size = 128
03/28/2023 16:17:46 - INFO - __main__ -   ***** Eval results feverous: ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-20000 *****
03/28/2023 16:17:46 - INFO - __main__ -     eval_loss = 2.051514148712158
03/28/2023 16:17:46 - INFO - __main__ -     eval_acc = 0.8103448275862069
03/28/2023 16:17:46 - INFO - __main__ -     eval_macro_f1 = 0.6094969025752005
03/28/2023 16:17:46 - INFO - __main__ -     eval_runtime = 21.1372
03/28/2023 16:17:46 - INFO - __main__ -     eval_samples_per_second = 167.383
03/28/2023 16:17:46 - INFO - __main__ -   ***** Best eval accuracy: 0.8157150932730356, '../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000' *****
03/28/2023 16:17:46 - INFO - transformers.configuration_utils -   loading configuration file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000/config.json
03/28/2023 16:17:46 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 16:17:46 - INFO - transformers.modeling_utils -   loading weights file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000/pytorch_model.bin
03/28/2023 16:17:47 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/28/2023 16:17:47 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/28/2023 16:17:47 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/28/2023 16:17:47 - INFO - __main__ -   Loading best model from ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000
03/28/2023 16:17:47 - INFO - transformers.configuration_utils -   loading configuration file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000/config.json
03/28/2023 16:17:47 - INFO - transformers.configuration_utils -   Model config AlbertConfig {
  "_name_or_path": "tals/albert-base-vitaminc-fever",
  "architectures": [
    "AlbertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": [
    "feverous"
  ],
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.3.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

03/28/2023 16:17:47 - INFO - transformers.modeling_utils -   loading weights file ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000/pytorch_model.bin
03/28/2023 16:17:47 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing AlbertForSequenceClassification.

03/28/2023 16:17:47 - INFO - transformers.modeling_utils -   All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ../../../../netscratch/bleick/feverous_train_test_real/checkpoint-16000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.
03/28/2023 16:17:47 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Collecting test examples (ratio=1.0) from dataset file at data/feverous
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   guid: 61142
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 416, 1378, 435, 288, 25, 21, 736, 636, 2558, 29107, 1, 27071, 1, 27071, 500, 500, 288, 432, 19, 15735, 1134, 136, 30, 16864, 19, 636, 2558, 25825, 4357, 1, 8542, 1, 6942, 1, 25825, 4357, 1026, 278, 500, 500, 9, 13, 3, 4652, 49, 13503, 4281, 3653, 257, 91, 1717, 26, 23473, 119, 416, 1378, 56, 25, 21, 736, 1652, 288, 432, 19, 15735, 1134, 136, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   guid: 67633
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 993, 9394, 18, 54, 2999, 43, 3401, 9083, 1480, 9477, 4091, 1236, 25, 21, 1236, 26, 9083, 522, 34, 636, 2558, 591, 6791, 18, 1, 108, 11602, 6601, 1, 591, 6791, 18, 4507, 500, 500, 26, 14, 636, 2558, 38, 79, 18, 12402, 1, 38, 79, 18, 12402, 500, 500, 13, 43, 3401, 190, 9, 1338, 4507, 3678, 13, 5, 1388, 6, 25, 40, 636, 2558, 12909, 1, 2031, 500, 500, 1099, 237, 9511, 19, 636, 2558, 12972, 472, 1, 12972, 472, 500, 500, 15, 636, 2558, 591, 6791, 18, 1, 591, 6791, 18, 500, 500, 15, 30, 4748, 17, 8332, 18, 636, 2558, 18, 7411, 27233, 1, 18, 7411, 27233, 18, 500, 500, 17, 617, 636, 2558, 12370, 1669, 1, 14022, 11193, 1, 12370, 1669, 17711, 500, 500, 15, 56, 32, 18339, 20, 7956, 12760, 17, 636, 2558, 177, 291, 16839, 911, 106, 1, 177, 291, 16839, 911, 445, 500, 500, 18861, 9, 13, 3, 993, 9394, 18, 15935, 50, 21, 1236, 26, 9083, 522, 679, 34, 1338, 4507, 15, 21, 237, 30, 8332, 18, 20249, 18, 17, 5547, 17711, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   guid: 68635
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 2009, 12820, 13, 5, 8621, 137, 15, 6829, 13, 10, 356, 1538, 15, 4643, 6, 23, 21, 8633, 17, 21, 1925, 2197, 15, 72, 23, 3395, 17, 841, 34, 21, 7380, 16762, 19, 636, 2558, 177, 29130, 103, 15, 1, 2681, 1, 2408, 7005, 1, 5788, 1, 177, 29130, 103, 15, 78, 305, 136, 500, 500, 9, 75, 1219, 20, 78, 305, 15, 24, 23, 2912, 16, 636, 2558, 2681, 1, 2408, 7005, 1, 26620, 1, 1041, 1, 7692, 1, 2681, 305, 314, 16, 232, 500, 500, 9, 13, 3, 2009, 12820, 15, 386, 27, 344, 137, 15, 6829, 15, 19, 7793, 15, 2040, 15, 23, 21, 1925, 2197, 19, 14, 78, 305, 314, 16, 232, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   guid: 2210
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 386, 19, 636, 2558, 1225, 3015, 1, 1225, 3015, 500, 500, 15, 636, 2558, 4749, 1, 1503, 2159, 99, 1, 4749, 1563, 500, 500, 20, 17702, 10471, 25862, 9939, 4794, 1897, 17, 33, 663, 15, 7726, 4072, 438, 7941, 4794, 1897, 27, 311, 547, 15, 7322, 15, 24, 1449, 636, 2558, 11157, 192, 1, 19153, 68, 1, 11157, 192, 1552, 500, 500, 15, 94, 260, 638, 28, 40, 1574, 17, 4103, 115, 1799, 29, 14, 636, 2558, 4749, 1, 512, 915, 1, 5, 4749, 1, 1503, 2159, 99, 6, 1, 12579, 2815, 548, 455, 500, 500, 112, 636, 2558, 4423, 1, 1885, 1, 49, 1, 4423, 176, 31, 500, 500, 9, 13, 3, 8858, 4794, 177, 23, 386, 19, 181, 1102, 27, 311, 547, 15, 1930, 17, 7379, 33, 2180, 545, 75, 14, 176, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   *** Example ***
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   guid: 35350
03/28/2023 16:17:51 - INFO - vitaminc.processing.utils -   features: InputFeatures(input_ids=[2, 33, 1144, 15, 12109, 2354, 351, 3405, 2655, 17, 11443, 6971, 15, 46, 20611, 5644, 9, 13, 3, 12109, 2354, 5937, 15, 72, 4533, 2115, 19630, 16056, 30, 46, 467, 19, 238, 7414, 15, 23, 411, 34, 14, 336, 437, 16, 14285, 20, 233, 3933, 20, 15620, 21, 360, 16, 4840, 642, 34, 14, 1396, 8, 1581, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
03/28/2023 16:17:53 - INFO - vitaminc.processing.multitask_sent_pair_cls -   Saving features into cached file data/cached_test_AlbertTokenizerFast_256_feverous_all [took 2.115 s]
03/28/2023 16:17:53 - INFO - transformers.trainer -   max_steps is given, it will override any value given in num_train_epochs
03/28/2023 16:17:53 - INFO - __main__ -   Evaluating on feverous
03/28/2023 16:17:53 - INFO - transformers.trainer -   ***** Running Evaluation *****
03/28/2023 16:17:53 - INFO - transformers.trainer -     Num examples = 5992
03/28/2023 16:17:53 - INFO - transformers.trainer -     Batch size = 128
03/28/2023 16:18:29 - INFO - __main__ -   ***** Test results feverous *****
03/28/2023 16:18:29 - INFO - __main__ -     eval_loss = 1.2256954908370972
03/28/2023 16:18:29 - INFO - __main__ -     eval_acc = 0.8791722296395194
03/28/2023 16:18:29 - INFO - __main__ -     eval_macro_f1 = 0.6282207093576688
03/28/2023 16:18:29 - INFO - __main__ -     eval_runtime = 35.6793
03/28/2023 16:18:29 - INFO - __main__ -     eval_samples_per_second = 167.941
